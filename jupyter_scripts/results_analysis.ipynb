{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(paths, weathers=[1,3,6,8]):\n",
    "    \"\"\"\n",
    "    We plot the summary of the testing for the set selected weathers.\n",
    "    We take the raw data and print the way\n",
    "    it was described on CORL 2017 paper\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Improve readability by adding a weather dictionary\n",
    "    weather_name_dict =  {1: 'Clear Noon', 3: 'After Rain Noon',\n",
    "                          6: 'Heavy Rain Noon', 8: 'Clear Sunset',\n",
    "                          4: 'Cloudy After Rain', 14: 'Soft Rain Sunset'}\n",
    "\n",
    "    # First we write the entire dictionary on the benchmark folder.\n",
    "    metrics_summary_lists = []\n",
    "    for path_ in paths:\n",
    "        with open(os.path.join(path_, 'metrics.json'), 'r') as fo:\n",
    "            metrics_summary = json.load(fo)\n",
    "            metrics_summary_lists.append(metrics_summary)\n",
    "\n",
    "    # Second we plot the metrics that are already ready by averaging\n",
    "    metrics_to_average = [\n",
    "        'episodes_fully_completed',\n",
    "        'episodes_completion'\n",
    "    ]\n",
    "\n",
    "    # We compute the number  of episodes based on size of average completion\n",
    "    number_of_episodes = len(\n",
    "        list(metrics_summary['episodes_fully_completed'].items())[0][1])\n",
    "\n",
    "    number_of_weathers = len(\n",
    "        list(metrics_summary['episodes_fully_completed'].items()))\n",
    "    \n",
    "    for metric in metrics_to_average:\n",
    "        if metric == 'episodes_completion':\n",
    "            print(\"Average Percentage of Distance to Goal Travelled \")\n",
    "        else:\n",
    "            print(\"Percentage of Successful Episodes\")\n",
    "        print(\"\")\n",
    "        \n",
    "        metric_sum_values = np.zeros(number_of_episodes)\n",
    "        for metric_count, metrics_summary in enumerate(metrics_summary_lists):\n",
    "            #print('  Metrics setup: ', paths[metric_count].split('/')[-1])\n",
    "            # metric_count = 0\n",
    "            values = metrics_summary[metric]\n",
    "            metric_sum_values_weathers = np.zeros(number_of_episodes)  # 4 different tasks\n",
    "            for weather, tasks in values.items():  # for testing there is only one weather\n",
    "                weather = int(float(weather))\n",
    "                if weather not in set(weathers):\n",
    "                    print(\"weather not exiset\")\n",
    "                    break\n",
    "                \n",
    "                #print('  Weather: ', weather_name_dict[weather])\n",
    "                for count, t in enumerate(tasks):\n",
    "                    # if isinstance(t, np.ndarray) or isinstance(t, list):\n",
    "                    if t == []:\n",
    "                        print('    Metric Not Computed')\n",
    "                    else:\n",
    "                        if number_of_weathers != 1:\n",
    "                            print('    ', metric_count,' Task:', count, ' -> ',\n",
    "                                float(sum(t)) / float(len(t)))\n",
    "                        metric_sum_values_weathers[count] += \\\n",
    "                            (float(sum(t)) / float(len(t))) * 1.0 / float(\n",
    "                                number_of_weathers)\n",
    "            \n",
    "            #print('  Average Between metrics')\n",
    "            for i in range(len(metric_sum_values_weathers)):\n",
    "                #print('    Task ', i, ' -> ', metric_sum_values_weathers[i])\n",
    "                metric_sum_values[i] += metric_sum_values_weathers[i] * 1.0 / float(len(metrics_summary_lists))\n",
    "            #print(\"\")\n",
    "\n",
    "        print('  Average Between metrics')\n",
    "        for i in range(len(metric_sum_values)):\n",
    "            print('    Task ', i, ' -> {:0.1f}'.format(metric_sum_values[i]*100))\n",
    "        print(\"\")\n",
    "\n",
    "    infraction_metrics = [\n",
    "        'intersection_otherlane',\n",
    "        'intersection_offroad',\n",
    "        'collision_other',\n",
    "        'collision_vehicles',\n",
    "        'collision_pedestrians'\n",
    "    ]\n",
    "\n",
    "    # We need to collect the total number of kilometers for each task\n",
    "\n",
    "    for metric in infraction_metrics:\n",
    "        \n",
    "        if metric == 'collision_pedestrians':\n",
    "            print('Avg. Kilometers driven before a collision to a PEDESTRIAN')\n",
    "        elif metric == 'collision_vehicles':\n",
    "            print('Avg. Kilometers driven before a collision to a VEHICLE')\n",
    "        elif metric == 'collision_other':\n",
    "            print('Avg. Kilometers driven before a collision to a STATIC OBSTACLE')\n",
    "        elif metric == 'intersection_offroad':\n",
    "            print('Avg. Kilometers driven before going OUTSIDE OF THE ROAD')\n",
    "        else:\n",
    "            print('Avg. Kilometers driven before invading the OPPOSITE LANE')\n",
    "            \n",
    "        metric_sum_values = np.zeros(number_of_episodes)        \n",
    "        summed_driven_kilometers = np.zeros(number_of_episodes)\n",
    "        for metric_count, metrics_summary in enumerate(metrics_summary_lists):\n",
    "            values_driven = metrics_summary['driven_kilometers']\n",
    "            values = metrics_summary[metric]\n",
    "\n",
    "            # print (zip(values.items(), values_driven.items()))\n",
    "            for items_metric, items_driven in zip(values.items(),\n",
    "                                              values_driven.items()):\n",
    "                weather = items_metric[0]\n",
    "                tasks = items_metric[1]\n",
    "                tasks_driven = items_driven[1]\n",
    "                weather = int(float(weather))\n",
    "                if weather not in set(weathers):\n",
    "                    print(\"weather not exiset\")\n",
    "                    break\n",
    "                #count = 0\n",
    "                for (count,t), t_driven in zip(enumerate(tasks), tasks_driven):\n",
    "                    # if isinstance(t, np.ndarray) or isinstance(t, list):\n",
    "                    if t == []:\n",
    "                        print('Metric Not Computed')\n",
    "                    else:\n",
    "                        if number_of_weathers != 1:\n",
    "                            if sum(t) > 0:\n",
    "                                print('    Task ', count,\n",
    "                                  ' -> ', t_driven / float(sum(t)))\n",
    "                            else:\n",
    "                                print('    Task ', count,\n",
    "                                  ' -> more than', t_driven)\n",
    "                        metric_sum_values[count] += float(sum(t))\n",
    "                        summed_driven_kilometers[count] += t_driven\n",
    "                        \n",
    "        #print('  Average Between metrics')\n",
    "        #for i in range(len(metric_sum_values)):\n",
    "        #    if metric_sum_values[i] == 0:\n",
    "        #        print('    Task ', i, ' -> more than ',\n",
    "        #             summed_driven_kilometers[i])\n",
    "        #   else:\n",
    "        #        print('    Task ', i, ' -> ',\n",
    "        #              summed_driven_kilometers[i] / metric_sum_values[i])\n",
    "        #print(\"\")\n",
    "        \n",
    "        print('  Average Between metrics')\n",
    "        if metric_sum_values[3] == 0:\n",
    "            print('    Task ', i, ' -> more than ',summed_driven_kilometers[i])\n",
    "        else:\n",
    "            print('    Task ', 3, ' -> {:0.2f}'.format(\n",
    "            summed_driven_kilometers[3] / metric_sum_values[3]))\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Successful Episodes\n",
      "\n",
      "  Average Between metrics\n",
      "    Task  0  -> 13.3\n",
      "    Task  1  -> 1.3\n",
      "    Task  2  -> 0.0\n",
      "    Task  3  -> 0.0\n",
      "\n",
      "Average Percentage of Distance to Goal Travelled \n",
      "\n",
      "  Average Between metrics\n",
      "    Task  0  -> 37.8\n",
      "    Task  1  -> 18.4\n",
      "    Task  2  -> 7.3\n",
      "    Task  3  -> 5.0\n",
      "\n",
      "Avg. Kilometers driven before invading the OPPOSITE LANE\n",
      "  Average Between metrics\n",
      "    Task  3  -> 0.23\n",
      "\n",
      "Avg. Kilometers driven before going OUTSIDE OF THE ROAD\n",
      "  Average Between metrics\n",
      "    Task  3  -> 0.21\n",
      "\n",
      "Avg. Kilometers driven before a collision to a STATIC OBSTACLE\n",
      "  Average Between metrics\n",
      "    Task  3  -> 0.14\n",
      "\n",
      "Avg. Kilometers driven before a collision to a VEHICLE\n",
      "  Average Between metrics\n",
      "    Task  3  -> 0.29\n",
      "\n",
      "Avg. Kilometers driven before a collision to a PEDESTRIAN\n",
      "  Average Between metrics\n",
      "    Task  3  -> 2.17\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/tai/ws/ijrr_2018/carla_env/CARLA_0.8.2/PythonClient/imitation_learning/_benchmarks_results/non_croped_policy/raw_testing/single_best/\"\n",
    "#path = \"/home/tai/ws/ijrr_2018/carla_env/CARLA_0.8.2/PythonClient/imitation_learning/_benchmarks_results/non_croped_policy/different_methods/domain_best_shift/\"\n",
    "\n",
    "path_list = [os.path.join(path, item) for item in os.listdir(path)]\n",
    "\n",
    "print_summary(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  infraction_metrics = [\n",
    "        'collision_pedestrians',\n",
    "        'collision_vehicles',\n",
    "        'collision_other',\n",
    "        'intersection_offroad',\n",
    "        'intersection_otherlane'\n",
    "\n",
    "    ]\n",
    "\n",
    "    # We need to collect the total number of kilometers for each task\n",
    "\n",
    "    for metric in infraction_metrics:\n",
    "        values_driven = metrics_summary['driven_kilometers']\n",
    "        values = metrics_summary[metric]\n",
    "        metric_sum_values = np.zeros(number_of_episodes)\n",
    "        summed_driven_kilometers = np.zeros(number_of_episodes)\n",
    "\n",
    "        if metric == 'collision_pedestrians':\n",
    "            print('Avg. Kilometers driven before a collision to a PEDESTRIAN')\n",
    "        elif metric == 'collision_vehicles':\n",
    "            print('Avg. Kilometers driven before a collision to a VEHICLE')\n",
    "        elif metric == 'collision_other':\n",
    "            print('Avg. Kilometers driven before a collision to a STATIC OBSTACLE')\n",
    "        elif metric == 'intersection_offroad':\n",
    "            print('Avg. Kilometers driven before going OUTSIDE OF THE ROAD')\n",
    "        else:\n",
    "            print('Avg. Kilometers driven before invading the OPPOSITE LANE')\n",
    "\n",
    "        # print (zip(values.items(), values_driven.items()))\n",
    "        for items_metric, items_driven in zip(values.items(),\n",
    "                                              values_driven.items()):\n",
    "            weather = items_metric[0]\n",
    "            tasks = items_metric[1]\n",
    "            tasks_driven = items_driven[1]\n",
    "            weather = int(float(weather))\n",
    "            if weather in set(weathers):\n",
    "                print('  Weather: ', weather_name_dict[weather])\n",
    "                count = 0\n",
    "                for t, t_driven in zip(tasks, tasks_driven):\n",
    "                    # if isinstance(t, np.ndarray) or isinstance(t, list):\n",
    "                    if t == []:\n",
    "                        print('Metric Not Computed')\n",
    "                    else:\n",
    "                        if sum(t) > 0:\n",
    "                            print('    Task ', count,\n",
    "                                  ' -> ', t_driven / float(sum(t)))\n",
    "                        else:\n",
    "                            print('    Task ', count,\n",
    "                                  ' -> more than', t_driven)\n",
    "\n",
    "                        metric_sum_values[count] += float(sum(t))\n",
    "                        summed_driven_kilometers[count] += t_driven\n",
    "\n",
    "                    count += 1\n",
    "        print('  Average Between Weathers')\n",
    "        for i in range(len(metric_sum_values)):\n",
    "            if metric_sum_values[i] == 0:\n",
    "                print('    Task ', i, ' -> more than ',\n",
    "                      summed_driven_kilometers[i])\n",
    "            else:\n",
    "                print('    Task ', i, ' -> ',\n",
    "                      summed_driven_kilometers[i] / metric_sum_values[i])\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
